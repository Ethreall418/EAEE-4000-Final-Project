{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13955287,"sourceType":"datasetVersion","datasetId":8895194}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"'''\nimporting a bunch of packages to make sure they are installed correctly\n'''\n\nimport sys, transformers, diffusers, matplotlib\nprint(sys.executable)\nprint(\"transformers\", transformers.__version__)\nprint(\"diffusers\", diffusers.__version__)\nprint('matplotlib from', matplotlib.__file__)\n\nfrom diffusers import UNet2DModel, DDPMScheduler, DDIMScheduler, DPMSolverMultistepScheduler\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-18T22:31:51.463943Z","iopub.execute_input":"2025-12-18T22:31:51.464246Z","iopub.status.idle":"2025-12-18T22:31:51.468865Z","shell.execute_reply.started":"2025-12-18T22:31:51.464227Z","shell.execute_reply":"2025-12-18T22:31:51.468200Z"}},"outputs":[{"name":"stdout","text":"/usr/bin/python3\ntransformers 4.53.3\ndiffusers 0.34.0\nmatplotlib from /usr/local/lib/python3.11/dist-packages/matplotlib/__init__.py\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import math\nimport random\nimport numpy as np\nimport xarray as xr\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset\nimport gc\nfrom datetime import datetime\nimport os\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\nDATA_PATH = '/kaggle/input/glorys-2014-2024/glorys_2014_2024 (1).nc'\nPRED_HORIZON = 7\nNUM_EPOCHS = 50\nBATCH_SIZE = 16\nADVECTION_DT = 1\nDIFFUSIVITY = 1e-3\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nSEED = 1234\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\nrandom.seed(SEED)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T22:33:10.046040Z","iopub.execute_input":"2025-12-18T22:33:10.046849Z","iopub.status.idle":"2025-12-18T22:33:10.054220Z","shell.execute_reply.started":"2025-12-18T22:33:10.046822Z","shell.execute_reply":"2025-12-18T22:33:10.053434Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"DEVICE","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T22:32:17.026728Z","iopub.execute_input":"2025-12-18T22:32:17.026986Z","iopub.status.idle":"2025-12-18T22:32:17.032328Z","shell.execute_reply.started":"2025-12-18T22:32:17.026969Z","shell.execute_reply":"2025-12-18T22:32:17.031638Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"ds = xr.open_dataset(DATA_PATH)\n\n\n# bigger subset for final project\nds = ds.isel(latitude=slice(200,200+256), longitude=slice(220,220+256))\nds = ds.coarsen(latitude=2, longitude=2, boundary=\"trim\").mean()\n\n# small subset for testing\n# ds = ds.isel(latitude=slice(200,220), longitude=slice(50,70))\n# NUM_EPOCHS = 5 \n# BATCH_SIZE = 4\n\nds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T22:33:13.393813Z","iopub.execute_input":"2025-12-18T22:33:13.394115Z","iopub.status.idle":"2025-12-18T22:35:17.739509Z","shell.execute_reply.started":"2025-12-18T22:33:13.394094Z","shell.execute_reply":"2025-12-18T22:35:17.738739Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"<xarray.Dataset> Size: 1GB\nDimensions:    (time: 3654, depth: 1, latitude: 128, longitude: 128)\nCoordinates:\n  * depth      (depth) float32 4B 0.494\n  * latitude   (latitude) float32 512B 24.71 24.88 25.04 ... 45.54 45.71 45.88\n  * longitude  (longitude) float32 512B -83.62 -83.46 -83.29 ... -62.62 -62.46\n  * time       (time) datetime64[ns] 29kB 2014-12-12 2014-12-13 ... 2024-12-12\nData variables:\n    thetao     (time, depth, latitude, longitude) float64 479MB 25.12 ... 5.482\n    uo         (time, depth, latitude, longitude) float64 479MB -0.006104 ......\n    vo         (time, depth, latitude, longitude) float64 479MB 0.0238 ... 0....\nAttributes:\n    references:                http://www.mercator-ocean.fr\n    title:                     daily mean fields from Global Ocean Physics An...\n    source:                    MERCATOR GLORYS12V1\n    institution:               MERCATOR OCEAN\n    Conventions:               CF-1.4\n    comment:                   CMEMS product\n    history:                   2023/06/01 16:20:05 MERCATOR OCEAN Netcdf crea...\n    copernicusmarine_version:  2.2.5","text/html":"<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n<defs>\n<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n</symbol>\n<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n</symbol>\n</defs>\n</svg>\n<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n *\n */\n\n:root {\n  --xr-font-color0: var(\n    --jp-content-font-color0,\n    var(--pst-color-text-base rgba(0, 0, 0, 1))\n  );\n  --xr-font-color2: var(\n    --jp-content-font-color2,\n    var(--pst-color-text-base, rgba(0, 0, 0, 0.54))\n  );\n  --xr-font-color3: var(\n    --jp-content-font-color3,\n    var(--pst-color-text-base, rgba(0, 0, 0, 0.38))\n  );\n  --xr-border-color: var(\n    --jp-border-color2,\n    hsl(from var(--pst-color-on-background, white) h s calc(l - 10))\n  );\n  --xr-disabled-color: var(\n    --jp-layout-color3,\n    hsl(from var(--pst-color-on-background, white) h s calc(l - 40))\n  );\n  --xr-background-color: var(\n    --jp-layout-color0,\n    var(--pst-color-on-background, white)\n  );\n  --xr-background-color-row-even: var(\n    --jp-layout-color1,\n    hsl(from var(--pst-color-on-background, white) h s calc(l - 5))\n  );\n  --xr-background-color-row-odd: var(\n    --jp-layout-color2,\n    hsl(from var(--pst-color-on-background, white) h s calc(l - 15))\n  );\n}\n\nhtml[theme=\"dark\"],\nhtml[data-theme=\"dark\"],\nbody[data-theme=\"dark\"],\nbody.vscode-dark {\n  --xr-font-color0: var(\n    --jp-content-font-color0,\n    var(--pst-color-text-base, rgba(255, 255, 255, 1))\n  );\n  --xr-font-color2: var(\n    --jp-content-font-color2,\n    var(--pst-color-text-base, rgba(255, 255, 255, 0.54))\n  );\n  --xr-font-color3: var(\n    --jp-content-font-color3,\n    var(--pst-color-text-base, rgba(255, 255, 255, 0.38))\n  );\n  --xr-border-color: var(\n    --jp-border-color2,\n    hsl(from var(--pst-color-on-background, #111111) h s calc(l + 10))\n  );\n  --xr-disabled-color: var(\n    --jp-layout-color3,\n    hsl(from var(--pst-color-on-background, #111111) h s calc(l + 40))\n  );\n  --xr-background-color: var(\n    --jp-layout-color0,\n    var(--pst-color-on-background, #111111)\n  );\n  --xr-background-color-row-even: var(\n    --jp-layout-color1,\n    hsl(from var(--pst-color-on-background, #111111) h s calc(l + 5))\n  );\n  --xr-background-color-row-odd: var(\n    --jp-layout-color2,\n    hsl(from var(--pst-color-on-background, #111111) h s calc(l + 15))\n  );\n}\n\n.xr-wrap {\n  display: block !important;\n  min-width: 300px;\n  max-width: 700px;\n}\n\n.xr-text-repr-fallback {\n  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n  display: none;\n}\n\n.xr-header {\n  padding-top: 6px;\n  padding-bottom: 6px;\n  margin-bottom: 4px;\n  border-bottom: solid 1px var(--xr-border-color);\n}\n\n.xr-header > div,\n.xr-header > ul {\n  display: inline;\n  margin-top: 0;\n  margin-bottom: 0;\n}\n\n.xr-obj-type,\n.xr-array-name {\n  margin-left: 2px;\n  margin-right: 10px;\n}\n\n.xr-obj-type {\n  color: var(--xr-font-color2);\n}\n\n.xr-sections {\n  padding-left: 0 !important;\n  display: grid;\n  grid-template-columns: 150px auto auto 1fr 0 20px 0 20px;\n}\n\n.xr-section-item {\n  display: contents;\n}\n\n.xr-section-item input {\n  display: inline-block;\n  opacity: 0;\n  height: 0;\n}\n\n.xr-section-item input + label {\n  color: var(--xr-disabled-color);\n  border: 2px solid transparent !important;\n}\n\n.xr-section-item input:enabled + label {\n  cursor: pointer;\n  color: var(--xr-font-color2);\n}\n\n.xr-section-item input:focus + label {\n  border: 2px solid var(--xr-font-color0) !important;\n}\n\n.xr-section-item input:enabled + label:hover {\n  color: var(--xr-font-color0);\n}\n\n.xr-section-summary {\n  grid-column: 1;\n  color: var(--xr-font-color2);\n  font-weight: 500;\n}\n\n.xr-section-summary > span {\n  display: inline-block;\n  padding-left: 0.5em;\n}\n\n.xr-section-summary-in:disabled + label {\n  color: var(--xr-font-color2);\n}\n\n.xr-section-summary-in + label:before {\n  display: inline-block;\n  content: \"►\";\n  font-size: 11px;\n  width: 15px;\n  text-align: center;\n}\n\n.xr-section-summary-in:disabled + label:before {\n  color: var(--xr-disabled-color);\n}\n\n.xr-section-summary-in:checked + label:before {\n  content: \"▼\";\n}\n\n.xr-section-summary-in:checked + label > span {\n  display: none;\n}\n\n.xr-section-summary,\n.xr-section-inline-details {\n  padding-top: 4px;\n  padding-bottom: 4px;\n}\n\n.xr-section-inline-details {\n  grid-column: 2 / -1;\n}\n\n.xr-section-details {\n  display: none;\n  grid-column: 1 / -1;\n  margin-bottom: 5px;\n}\n\n.xr-section-summary-in:checked ~ .xr-section-details {\n  display: contents;\n}\n\n.xr-array-wrap {\n  grid-column: 1 / -1;\n  display: grid;\n  grid-template-columns: 20px auto;\n}\n\n.xr-array-wrap > label {\n  grid-column: 1;\n  vertical-align: top;\n}\n\n.xr-preview {\n  color: var(--xr-font-color3);\n}\n\n.xr-array-preview,\n.xr-array-data {\n  padding: 0 5px !important;\n  grid-column: 2;\n}\n\n.xr-array-data,\n.xr-array-in:checked ~ .xr-array-preview {\n  display: none;\n}\n\n.xr-array-in:checked ~ .xr-array-data,\n.xr-array-preview {\n  display: inline-block;\n}\n\n.xr-dim-list {\n  display: inline-block !important;\n  list-style: none;\n  padding: 0 !important;\n  margin: 0;\n}\n\n.xr-dim-list li {\n  display: inline-block;\n  padding: 0;\n  margin: 0;\n}\n\n.xr-dim-list:before {\n  content: \"(\";\n}\n\n.xr-dim-list:after {\n  content: \")\";\n}\n\n.xr-dim-list li:not(:last-child):after {\n  content: \",\";\n  padding-right: 5px;\n}\n\n.xr-has-index {\n  font-weight: bold;\n}\n\n.xr-var-list,\n.xr-var-item {\n  display: contents;\n}\n\n.xr-var-item > div,\n.xr-var-item label,\n.xr-var-item > .xr-var-name span {\n  background-color: var(--xr-background-color-row-even);\n  border-color: var(--xr-background-color-row-odd);\n  margin-bottom: 0;\n  padding-top: 2px;\n}\n\n.xr-var-item > .xr-var-name:hover span {\n  padding-right: 5px;\n}\n\n.xr-var-list > li:nth-child(odd) > div,\n.xr-var-list > li:nth-child(odd) > label,\n.xr-var-list > li:nth-child(odd) > .xr-var-name span {\n  background-color: var(--xr-background-color-row-odd);\n  border-color: var(--xr-background-color-row-even);\n}\n\n.xr-var-name {\n  grid-column: 1;\n}\n\n.xr-var-dims {\n  grid-column: 2;\n}\n\n.xr-var-dtype {\n  grid-column: 3;\n  text-align: right;\n  color: var(--xr-font-color2);\n}\n\n.xr-var-preview {\n  grid-column: 4;\n}\n\n.xr-index-preview {\n  grid-column: 2 / 5;\n  color: var(--xr-font-color2);\n}\n\n.xr-var-name,\n.xr-var-dims,\n.xr-var-dtype,\n.xr-preview,\n.xr-attrs dt {\n  white-space: nowrap;\n  overflow: hidden;\n  text-overflow: ellipsis;\n  padding-right: 10px;\n}\n\n.xr-var-name:hover,\n.xr-var-dims:hover,\n.xr-var-dtype:hover,\n.xr-attrs dt:hover {\n  overflow: visible;\n  width: auto;\n  z-index: 1;\n}\n\n.xr-var-attrs,\n.xr-var-data,\n.xr-index-data {\n  display: none;\n  border-top: 2px dotted var(--xr-background-color);\n  padding-bottom: 20px !important;\n  padding-top: 10px !important;\n}\n\n.xr-var-attrs-in + label,\n.xr-var-data-in + label,\n.xr-index-data-in + label {\n  padding: 0 1px;\n}\n\n.xr-var-attrs-in:checked ~ .xr-var-attrs,\n.xr-var-data-in:checked ~ .xr-var-data,\n.xr-index-data-in:checked ~ .xr-index-data {\n  display: block;\n}\n\n.xr-var-data > table {\n  float: right;\n}\n\n.xr-var-data > pre,\n.xr-index-data > pre,\n.xr-var-data > table > tbody > tr {\n  background-color: transparent !important;\n}\n\n.xr-var-name span,\n.xr-var-data,\n.xr-index-name div,\n.xr-index-data,\n.xr-attrs {\n  padding-left: 25px !important;\n}\n\n.xr-attrs,\n.xr-var-attrs,\n.xr-var-data,\n.xr-index-data {\n  grid-column: 1 / -1;\n}\n\ndl.xr-attrs {\n  padding: 0;\n  margin: 0;\n  display: grid;\n  grid-template-columns: 125px auto;\n}\n\n.xr-attrs dt,\n.xr-attrs dd {\n  padding: 0;\n  margin: 0;\n  float: left;\n  padding-right: 10px;\n  width: auto;\n}\n\n.xr-attrs dt {\n  font-weight: normal;\n  grid-column: 1;\n}\n\n.xr-attrs dt:hover span {\n  display: inline-block;\n  background: var(--xr-background-color);\n  padding-right: 10px;\n}\n\n.xr-attrs dd {\n  grid-column: 2;\n  white-space: pre-wrap;\n  word-break: break-all;\n}\n\n.xr-icon-database,\n.xr-icon-file-text2,\n.xr-no-icon {\n  display: inline-block;\n  vertical-align: middle;\n  width: 1em;\n  height: 1.5em !important;\n  stroke-width: 0;\n  stroke: currentColor;\n  fill: currentColor;\n}\n\n.xr-var-attrs-in:checked + label > .xr-icon-file-text2,\n.xr-var-data-in:checked + label > .xr-icon-database,\n.xr-index-data-in:checked + label > .xr-icon-database {\n  color: var(--xr-font-color0);\n  filter: drop-shadow(1px 1px 5px var(--xr-font-color2));\n  stroke-width: 0.8px;\n}\n</style><pre class='xr-text-repr-fallback'>&lt;xarray.Dataset&gt; Size: 1GB\nDimensions:    (time: 3654, depth: 1, latitude: 128, longitude: 128)\nCoordinates:\n  * depth      (depth) float32 4B 0.494\n  * latitude   (latitude) float32 512B 24.71 24.88 25.04 ... 45.54 45.71 45.88\n  * longitude  (longitude) float32 512B -83.62 -83.46 -83.29 ... -62.62 -62.46\n  * time       (time) datetime64[ns] 29kB 2014-12-12 2014-12-13 ... 2024-12-12\nData variables:\n    thetao     (time, depth, latitude, longitude) float64 479MB 25.12 ... 5.482\n    uo         (time, depth, latitude, longitude) float64 479MB -0.006104 ......\n    vo         (time, depth, latitude, longitude) float64 479MB 0.0238 ... 0....\nAttributes:\n    references:                http://www.mercator-ocean.fr\n    title:                     daily mean fields from Global Ocean Physics An...\n    source:                    MERCATOR GLORYS12V1\n    institution:               MERCATOR OCEAN\n    Conventions:               CF-1.4\n    comment:                   CMEMS product\n    history:                   2023/06/01 16:20:05 MERCATOR OCEAN Netcdf crea...\n    copernicusmarine_version:  2.2.5</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.Dataset</div></div><ul class='xr-sections'><li class='xr-section-item'><input id='section-23fd2268-87af-4955-bb44-b07134e98032' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-23fd2268-87af-4955-bb44-b07134e98032' class='xr-section-summary'  title='Expand/collapse section'>Dimensions:</label><div class='xr-section-inline-details'><ul class='xr-dim-list'><li><span class='xr-has-index'>time</span>: 3654</li><li><span class='xr-has-index'>depth</span>: 1</li><li><span class='xr-has-index'>latitude</span>: 128</li><li><span class='xr-has-index'>longitude</span>: 128</li></ul></div><div class='xr-section-details'></div></li><li class='xr-section-item'><input id='section-6ac8f7ff-ad6f-423a-a4bd-2734411f6600' class='xr-section-summary-in' type='checkbox'  checked><label for='section-6ac8f7ff-ad6f-423a-a4bd-2734411f6600' class='xr-section-summary' >Coordinates: <span>(4)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>depth</span></div><div class='xr-var-dims'>(depth)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>0.494</div><input id='attrs-fe4385f3-30fb-4d9b-aea7-e4445d662176' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-fe4385f3-30fb-4d9b-aea7-e4445d662176' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-845350f9-7000-4938-9fbc-d01033a86be2' class='xr-var-data-in' type='checkbox'><label for='data-845350f9-7000-4938-9fbc-d01033a86be2' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>units :</span></dt><dd>m</dd><dt><span>standard_name :</span></dt><dd>depth</dd><dt><span>long_name :</span></dt><dd>Depth</dd><dt><span>positive :</span></dt><dd>down</dd><dt><span>unit_long :</span></dt><dd>Meters</dd><dt><span>axis :</span></dt><dd>Z</dd></dl></div><div class='xr-var-data'><pre>array([0.494025], dtype=float32)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>latitude</span></div><div class='xr-var-dims'>(latitude)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>24.71 24.88 25.04 ... 45.71 45.88</div><input id='attrs-bb276e68-57cb-4c91-8281-b9e9cf0a4c30' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-bb276e68-57cb-4c91-8281-b9e9cf0a4c30' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-6085bd58-aa1e-4c1f-9553-42de5c618413' class='xr-var-data-in' type='checkbox'><label for='data-6085bd58-aa1e-4c1f-9553-42de5c618413' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>units :</span></dt><dd>degrees_north</dd><dt><span>standard_name :</span></dt><dd>latitude</dd><dt><span>long_name :</span></dt><dd>Latitude</dd><dt><span>unit_long :</span></dt><dd>Degrees North</dd><dt><span>axis :</span></dt><dd>Y</dd></dl></div><div class='xr-var-data'><pre>array([24.708332, 24.875   , 25.041668, 25.208332, 25.375   , 25.541668,\n       25.708332, 25.875   , 26.041668, 26.208332, 26.375   , 26.541668,\n       26.708332, 26.875   , 27.041668, 27.208332, 27.375   , 27.541668,\n       27.708332, 27.875   , 28.041668, 28.208332, 28.375   , 28.541668,\n       28.708332, 28.875   , 29.041668, 29.208332, 29.375   , 29.541668,\n       29.708332, 29.875   , 30.041668, 30.208332, 30.375   , 30.541668,\n       30.708332, 30.875   , 31.041668, 31.208332, 31.375   , 31.541668,\n       31.708332, 31.875   , 32.041664, 32.208336, 32.375   , 32.541664,\n       32.708336, 32.875   , 33.041664, 33.208336, 33.375   , 33.541664,\n       33.708336, 33.875   , 34.041664, 34.208336, 34.375   , 34.541664,\n       34.708336, 34.875   , 35.041664, 35.208336, 35.375   , 35.541664,\n       35.708336, 35.875   , 36.041664, 36.208336, 36.375   , 36.541664,\n       36.708336, 36.875   , 37.041664, 37.208336, 37.375   , 37.541664,\n       37.708336, 37.875   , 38.041664, 38.208336, 38.375   , 38.541664,\n       38.708336, 38.875   , 39.041664, 39.208336, 39.375   , 39.541664,\n       39.708336, 39.875   , 40.041664, 40.208336, 40.375   , 40.541664,\n       40.708336, 40.875   , 41.041664, 41.208336, 41.375   , 41.541664,\n       41.708336, 41.875   , 42.041664, 42.208336, 42.375   , 42.541664,\n       42.708336, 42.875   , 43.041664, 43.208336, 43.375   , 43.541664,\n       43.708336, 43.875   , 44.041664, 44.208336, 44.375   , 44.541664,\n       44.708336, 44.875   , 45.041664, 45.208336, 45.375   , 45.541664,\n       45.708336, 45.875   ], dtype=float32)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>longitude</span></div><div class='xr-var-dims'>(longitude)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>-83.62 -83.46 ... -62.62 -62.46</div><input id='attrs-6a323561-05bf-4a74-86b1-53238bd16e22' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-6a323561-05bf-4a74-86b1-53238bd16e22' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-4d3e2259-a0e2-4159-9d08-8fd01cf8d93f' class='xr-var-data-in' type='checkbox'><label for='data-4d3e2259-a0e2-4159-9d08-8fd01cf8d93f' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>units :</span></dt><dd>degrees_east</dd><dt><span>standard_name :</span></dt><dd>longitude</dd><dt><span>long_name :</span></dt><dd>Longitude</dd><dt><span>unit_long :</span></dt><dd>Degrees East</dd><dt><span>axis :</span></dt><dd>X</dd></dl></div><div class='xr-var-data'><pre>array([-83.625   , -83.45833 , -83.29167 , -83.125   , -82.95833 , -82.79167 ,\n       -82.625   , -82.45833 , -82.29167 , -82.125   , -81.95833 , -81.79167 ,\n       -81.625   , -81.45833 , -81.29167 , -81.125   , -80.95833 , -80.79167 ,\n       -80.625   , -80.45833 , -80.29167 , -80.125   , -79.95833 , -79.79167 ,\n       -79.625   , -79.45833 , -79.29167 , -79.125   , -78.95833 , -78.79167 ,\n       -78.625   , -78.45833 , -78.29167 , -78.125   , -77.95833 , -77.79167 ,\n       -77.625   , -77.45833 , -77.29167 , -77.125   , -76.95833 , -76.79167 ,\n       -76.625   , -76.45833 , -76.29167 , -76.125   , -75.95833 , -75.79167 ,\n       -75.625   , -75.45833 , -75.29167 , -75.125   , -74.95833 , -74.79167 ,\n       -74.625   , -74.45833 , -74.29167 , -74.125   , -73.95833 , -73.79167 ,\n       -73.625   , -73.45833 , -73.29167 , -73.125   , -72.95833 , -72.79167 ,\n       -72.625   , -72.45833 , -72.29167 , -72.125   , -71.95833 , -71.79167 ,\n       -71.625   , -71.45833 , -71.29167 , -71.125   , -70.95833 , -70.79167 ,\n       -70.625   , -70.45833 , -70.29167 , -70.125   , -69.95833 , -69.79167 ,\n       -69.625   , -69.45833 , -69.29167 , -69.125   , -68.95833 , -68.79167 ,\n       -68.625   , -68.45833 , -68.29167 , -68.125   , -67.95833 , -67.79167 ,\n       -67.625   , -67.45833 , -67.29167 , -67.125   , -66.95833 , -66.79167 ,\n       -66.625   , -66.45833 , -66.29167 , -66.125   , -65.95833 , -65.79167 ,\n       -65.625   , -65.45833 , -65.29167 , -65.125   , -64.95833 , -64.79167 ,\n       -64.625   , -64.45833 , -64.29167 , -64.125   , -63.958336, -63.791664,\n       -63.625   , -63.458336, -63.291664, -63.125   , -62.958336, -62.791664,\n       -62.625   , -62.458336], dtype=float32)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>time</span></div><div class='xr-var-dims'>(time)</div><div class='xr-var-dtype'>datetime64[ns]</div><div class='xr-var-preview xr-preview'>2014-12-12 ... 2024-12-12</div><input id='attrs-8fea9093-8432-42da-be87-0ae3bd5a5517' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-8fea9093-8432-42da-be87-0ae3bd5a5517' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-5bfa9191-34f8-4da5-8f00-50a854b62f9c' class='xr-var-data-in' type='checkbox'><label for='data-5bfa9191-34f8-4da5-8f00-50a854b62f9c' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>axis :</span></dt><dd>T</dd><dt><span>unit_long :</span></dt><dd>Hours Since 1950-01-01</dd><dt><span>standard_name :</span></dt><dd>time</dd><dt><span>long_name :</span></dt><dd>Time</dd></dl></div><div class='xr-var-data'><pre>array([&#x27;2014-12-12T00:00:00.000000000&#x27;, &#x27;2014-12-13T00:00:00.000000000&#x27;,\n       &#x27;2014-12-14T00:00:00.000000000&#x27;, ..., &#x27;2024-12-10T00:00:00.000000000&#x27;,\n       &#x27;2024-12-11T00:00:00.000000000&#x27;, &#x27;2024-12-12T00:00:00.000000000&#x27;],\n      dtype=&#x27;datetime64[ns]&#x27;)</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-79f8ed85-c7e0-43ed-affa-38fa5060409e' class='xr-section-summary-in' type='checkbox'  checked><label for='section-79f8ed85-c7e0-43ed-affa-38fa5060409e' class='xr-section-summary' >Data variables: <span>(3)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>thetao</span></div><div class='xr-var-dims'>(time, depth, latitude, longitude)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>25.12 25.41 25.47 ... 4.808 5.482</div><input id='attrs-803908e0-9f64-42fb-839f-8cb94b8a6dee' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-803908e0-9f64-42fb-839f-8cb94b8a6dee' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-4af46c8e-a613-4715-b603-3f095f3d8ddc' class='xr-var-data-in' type='checkbox'><label for='data-4af46c8e-a613-4715-b603-3f095f3d8ddc' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>units :</span></dt><dd>degrees_C</dd><dt><span>valid_min :</span></dt><dd>-32766</dd><dt><span>standard_name :</span></dt><dd>sea_water_potential_temperature</dd><dt><span>long_name :</span></dt><dd>Temperature</dd><dt><span>unit_long :</span></dt><dd>Degrees Celsius</dd><dt><span>valid_max :</span></dt><dd>21306</dd></dl></div><div class='xr-var-data'><pre>array([[[[25.120365  , 25.41425824, 25.46900845, ..., 26.73357341,\n          26.67461165, 26.62883388],\n         [25.1436201 , 25.37122715, 25.39576403, ..., 26.69457075,\n          26.62663655, 26.57756279],\n         [25.01452681, 25.2595294 , 25.34339426, ..., 26.67625965,\n          26.61931211, 26.56071657],\n         ...,\n         [        nan,         nan,         nan, ...,         nan,\n                  nan,         nan],\n         [        nan,         nan,         nan, ...,  4.51634268,\n           4.88000124,  5.31489   ],\n         [        nan,         nan,         nan, ...,  5.04095585,\n           5.10760828,  5.54084904]]],\n\n\n       [[[25.05426191, 25.36902981, 25.40968047, ..., 26.65117343,\n          26.60905789, 26.58122501],\n         [25.08630634, 25.33552049, 25.31611072, ..., 26.60045167,\n          26.54972991, 26.51878414],\n         [24.96618549, 25.22565386, 25.27344584, ..., 26.5777459 ,\n...\n                  nan,         nan],\n         [        nan,         nan,         nan, ...,  4.13840146,\n           4.26291697,  4.67162085],\n         [        nan,         nan,         nan, ...,  4.18198189,\n           4.4670858 ,  5.3233131 ]]],\n\n\n       [[[24.92864772, 24.58403271, 24.5258034 , ..., 26.45908993,\n          26.42027039, 26.246498  ],\n         [24.97753837, 24.59794915, 24.40678121, ..., 26.44828638,\n          26.32981353, 26.10330515],\n         [24.84514908, 24.50383007, 24.32273323, ..., 26.28476821,\n          26.11850337, 25.96487319],\n         ...,\n         [        nan,         nan,         nan, ...,         nan,\n                  nan,         nan],\n         [        nan,         nan,         nan, ...,  4.33030184,\n           4.65147864,  4.92669456],\n         [        nan,         nan,         nan, ...,  4.48484758,\n           4.80803859,  5.48243661]]]])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>uo</span></div><div class='xr-var-dims'>(time, depth, latitude, longitude)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>-0.006104 -0.01938 ... -0.07447</div><input id='attrs-b0eecd52-15cb-47e4-879e-1da08337fe00' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-b0eecd52-15cb-47e4-879e-1da08337fe00' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-6ef03f02-e2d9-46ad-bc55-d3a26223eaee' class='xr-var-data-in' type='checkbox'><label for='data-6ef03f02-e2d9-46ad-bc55-d3a26223eaee' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>units :</span></dt><dd>m s-1</dd><dt><span>valid_min :</span></dt><dd>-3123</dd><dt><span>standard_name :</span></dt><dd>eastward_sea_water_velocity</dd><dt><span>long_name :</span></dt><dd>Eastward velocity</dd><dt><span>unit_long :</span></dt><dd>Meters per second</dd><dt><span>valid_max :</span></dt><dd>4314</dd></dl></div><div class='xr-var-data'><pre>array([[[[-6.10370189e-03, -1.93792535e-02, -1.11392559e-02, ...,\n          -4.39466536e-02, -3.83007294e-02, -3.95214697e-02],\n         [ 1.00711081e-02, -6.71407208e-03, -7.17184972e-03, ...,\n          -6.36310922e-02, -7.32444227e-02, -8.31629382e-02],\n         [ 1.78533280e-02,  1.37333293e-03,  1.80059206e-02, ...,\n          -8.25525681e-02, -8.19421979e-02, -7.69066438e-02],\n         ...,\n         [            nan,             nan,             nan, ...,\n                      nan,             nan,             nan],\n         [            nan,             nan,             nan, ...,\n           1.15970336e-02, -1.25125889e-02, -8.13318277e-02],\n         [            nan,             nan,             nan, ...,\n          -8.31629382e-02, -3.47911008e-02,  5.64592425e-03]]],\n\n\n       [[[-2.86873989e-02, -5.21866512e-02, -4.01318399e-02, ...,\n          -1.02237007e-02, -9.15555283e-04,  3.66222113e-03],\n         [-2.76192511e-02, -4.22681356e-02, -4.18103579e-02, ...,\n          -3.11288796e-02, -3.89110995e-02, -5.03555406e-02],\n         [-1.77007355e-02, -3.17392498e-02, -1.96844386e-02, ...,\n...\n                      nan,             nan,             nan],\n         [            nan,             nan,             nan, ...,\n          -5.49333170e-02, -8.66725668e-02, -7.24814599e-02],\n         [            nan,             nan,             nan, ...,\n          -3.12814722e-02, -1.09866634e-02, -7.32444227e-03]]],\n\n\n       [[[-1.43131809e-01, -1.35349589e-01, -1.16428114e-01, ...,\n          -4.36414685e-02, -5.95110934e-02, -7.11081270e-02],\n         [-1.16275521e-01, -1.08188116e-01, -1.06509598e-01, ...,\n          -3.28073977e-02, -4.07422101e-02, -5.73747978e-02],\n         [-9.97955259e-02, -1.02999969e-01, -1.02847377e-01, ...,\n          -2.41096225e-02, -2.73140660e-02, -7.17184972e-02],\n         ...,\n         [            nan,             nan,             nan, ...,\n                      nan,             nan,             nan],\n         [            nan,             nan,             nan, ...,\n          -2.92977691e-02, -1.10782189e-01, -1.63579211e-01],\n         [            nan,             nan,             nan, ...,\n          -1.89367351e-01, -1.22531815e-01, -7.44651631e-02]]]])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>vo</span></div><div class='xr-var-dims'>(time, depth, latitude, longitude)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>0.0238 0.01434 ... 0.1466 0.1721</div><input id='attrs-d86910ca-d0fb-42a7-9c32-d98fb872cf97' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-d86910ca-d0fb-42a7-9c32-d98fb872cf97' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-176f9c6a-689d-461f-9397-a4cc186bebab' class='xr-var-data-in' type='checkbox'><label for='data-176f9c6a-689d-461f-9397-a4cc186bebab' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>units :</span></dt><dd>m s-1</dd><dt><span>valid_min :</span></dt><dd>-3680</dd><dt><span>standard_name :</span></dt><dd>northward_sea_water_velocity</dd><dt><span>long_name :</span></dt><dd>Northward velocity</dd><dt><span>unit_long :</span></dt><dd>Meters per second</dd><dt><span>valid_max :</span></dt><dd>3639</dd></dl></div><div class='xr-var-data'><pre>array([[[[ 0.02380444,  0.0143437 , -0.02502518, ..., -0.04242073,\n          -0.00259407,  0.03143406],\n         [ 0.05310221,  0.02685629, -0.0224311 , ..., -0.04348888,\n          -0.00457778,  0.03021332],\n         [ 0.07141331,  0.04272591, -0.01525925, ..., -0.04058962,\n          -0.00045778,  0.04074221],\n         ...,\n         [        nan,         nan,         nan, ...,         nan,\n                  nan,         nan],\n         [        nan,         nan,         nan, ...,  0.03845332,\n           0.10376293,  0.0634785 ],\n         [        nan,         nan,         nan, ...,  0.00961333,\n           0.07660146,  0.14084292]]],\n\n\n       [[[ 0.01693777,  0.02945036, -0.00518815, ..., -0.04379406,\n           0.00259407,  0.0389111 ],\n         [ 0.05859554,  0.03662221, -0.00701926, ..., -0.0450148 ,\n          -0.00350963,  0.03234962],\n         [ 0.08041627,  0.0511185 ,  0.00137333, ..., -0.04379406,\n...\n                  nan,         nan],\n         [        nan,         nan,         nan, ...,  0.08362072,\n           0.10803552,  0.02609333],\n         [        nan,         nan,         nan, ...,  0.04989776,\n           0.07705924,  0.04074221]]],\n\n\n       [[[-0.13458663, -0.135197  , -0.1371807 , ..., -0.03402814,\n          -0.09689627, -0.13397626],\n         [-0.14267403, -0.1452681 , -0.14618366, ..., -0.0286874 ,\n          -0.08072146, -0.09186071],\n         [-0.15091403, -0.14511551, -0.139317  , ..., -0.01861629,\n          -0.05035554, -0.02960295],\n         ...,\n         [        nan,         nan,         nan, ...,         nan,\n                  nan,         nan],\n         [        nan,         nan,         nan, ...,  0.1208533 ,\n           0.21790216,  0.0430311 ],\n         [        nan,         nan,         nan, ...,  0.02700888,\n           0.14664144,  0.17212439]]]])</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-6ff29d0a-aa17-495a-a3b9-3616c2096cd0' class='xr-section-summary-in' type='checkbox'  ><label for='section-6ff29d0a-aa17-495a-a3b9-3616c2096cd0' class='xr-section-summary' >Indexes: <span>(4)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-index-name'><div>depth</div></div><div class='xr-index-preview'>PandasIndex</div><input type='checkbox' disabled/><label></label><input id='index-73852187-c13d-42c5-9bd1-fecc7df97f83' class='xr-index-data-in' type='checkbox'/><label for='index-73852187-c13d-42c5-9bd1-fecc7df97f83' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([0.494025], dtype=&#x27;float32&#x27;, name=&#x27;depth&#x27;))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>latitude</div></div><div class='xr-index-preview'>PandasIndex</div><input type='checkbox' disabled/><label></label><input id='index-a4f6f0d7-b6dc-4d8e-a14c-504bebe53662' class='xr-index-data-in' type='checkbox'/><label for='index-a4f6f0d7-b6dc-4d8e-a14c-504bebe53662' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([24.708332061767578,             24.875, 25.041667938232422,\n       25.208332061767578,             25.375, 25.541667938232422,\n       25.708332061767578,             25.875, 26.041667938232422,\n       26.208332061767578,\n       ...\n                   44.375, 44.541664123535156, 44.708335876464844,\n                   44.875, 45.041664123535156, 45.208335876464844,\n                   45.375, 45.541664123535156, 45.708335876464844,\n                   45.875],\n      dtype=&#x27;float32&#x27;, name=&#x27;latitude&#x27;, length=128))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>longitude</div></div><div class='xr-index-preview'>PandasIndex</div><input type='checkbox' disabled/><label></label><input id='index-4fcd54fc-86d1-45d4-86e6-44c8b40a30ca' class='xr-index-data-in' type='checkbox'/><label for='index-4fcd54fc-86d1-45d4-86e6-44c8b40a30ca' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([            -83.625,  -83.45832824707031,  -83.29167175292969,\n                   -83.125,  -82.95832824707031,  -82.79167175292969,\n                   -82.625,  -82.45832824707031,  -82.29167175292969,\n                   -82.125,\n       ...\n       -63.958335876464844, -63.791664123535156,             -63.625,\n       -63.458335876464844, -63.291664123535156,             -63.125,\n       -62.958335876464844, -62.791664123535156,             -62.625,\n       -62.458335876464844],\n      dtype=&#x27;float32&#x27;, name=&#x27;longitude&#x27;, length=128))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>time</div></div><div class='xr-index-preview'>PandasIndex</div><input type='checkbox' disabled/><label></label><input id='index-8c7140f3-9785-4ca0-abb1-a6c2f56785f5' class='xr-index-data-in' type='checkbox'/><label for='index-8c7140f3-9785-4ca0-abb1-a6c2f56785f5' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(DatetimeIndex([&#x27;2014-12-12&#x27;, &#x27;2014-12-13&#x27;, &#x27;2014-12-14&#x27;, &#x27;2014-12-15&#x27;,\n               &#x27;2014-12-16&#x27;, &#x27;2014-12-17&#x27;, &#x27;2014-12-18&#x27;, &#x27;2014-12-19&#x27;,\n               &#x27;2014-12-20&#x27;, &#x27;2014-12-21&#x27;,\n               ...\n               &#x27;2024-12-03&#x27;, &#x27;2024-12-04&#x27;, &#x27;2024-12-05&#x27;, &#x27;2024-12-06&#x27;,\n               &#x27;2024-12-07&#x27;, &#x27;2024-12-08&#x27;, &#x27;2024-12-09&#x27;, &#x27;2024-12-10&#x27;,\n               &#x27;2024-12-11&#x27;, &#x27;2024-12-12&#x27;],\n              dtype=&#x27;datetime64[ns]&#x27;, name=&#x27;time&#x27;, length=3654, freq=None))</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-fb17d9dc-c261-45b9-a086-fc79e9323620' class='xr-section-summary-in' type='checkbox'  checked><label for='section-fb17d9dc-c261-45b9-a086-fc79e9323620' class='xr-section-summary' >Attributes: <span>(8)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'><dt><span>references :</span></dt><dd>http://www.mercator-ocean.fr</dd><dt><span>title :</span></dt><dd>daily mean fields from Global Ocean Physics Analysis and Forecast updated Daily</dd><dt><span>source :</span></dt><dd>MERCATOR GLORYS12V1</dd><dt><span>institution :</span></dt><dd>MERCATOR OCEAN</dd><dt><span>Conventions :</span></dt><dd>CF-1.4</dd><dt><span>comment :</span></dt><dd>CMEMS product</dd><dt><span>history :</span></dt><dd>2023/06/01 16:20:05 MERCATOR OCEAN Netcdf creation</dd><dt><span>copernicusmarine_version :</span></dt><dd>2.2.5</dd></dl></div></li></ul></div></div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"\n# ---- Raw data with NaNs kept ----\nsea_temp_raw = ds[\"thetao\"].isel(depth=0).values.astype(np.float32)  # (time, lat, lon)\nu_field_raw  = ds[\"uo\"].isel(depth=0).values.astype(np.float32)\nv_field_raw  = ds[\"vo\"].isel(depth=0).values.astype(np.float32)\n\n# ---- Common validity mask: valid only where ALL three fields are finite ----\nvalid_3d = (\n    np.isfinite(sea_temp_raw) &\n    np.isfinite(u_field_raw) &\n    np.isfinite(v_field_raw)\n)  # shape: (time, lat, lon)\n\n# require a grid cell to be valid for ALL selected times\nvalid_2d = valid_3d.all(axis=0).astype(np.float32)   # (lat, lon)\n\nprint(\"Total spatial pixels:\", valid_2d.size)\nprint(\"Valid ocean pixels (common mask):\", valid_2d.sum())\n\n# Single global mask for everything: shape (1, 1, H, W)\nVALID_MASK = torch.tensor(valid_2d, dtype=torch.float32, device=DEVICE).unsqueeze(0).unsqueeze(0)\n\n# ---- Fill NaNs for model inputs (we'll still mask them in loss) ----\nsea_temp = np.nan_to_num(sea_temp_raw, nan=0.0, posinf=0.0, neginf=0.0)\nu_field  = np.nan_to_num(u_field_raw,  nan=0.0, posinf=0.0, neginf=0.0)\nv_field  = np.nan_to_num(v_field_raw,  nan=0.0, posinf=0.0, neginf=0.0)\n\n# ---- Compute statistics using ONLY valid ocean points ----\ntrain_count = int((sea_temp.shape[0] - PRED_HORIZON - 1) * 0.7)\ntemp_mean = sea_temp_raw[valid_3d][:train_count].mean()\ntemp_std  = sea_temp_raw[valid_3d][:train_count].std() + 1e-6\n\nvel_stack = np.stack([u_field_raw, v_field_raw], axis=0)  # (2, time, lat, lon)\nvel_valid = np.isfinite(vel_stack) & np.broadcast_to(valid_3d, vel_stack.shape)\nvel_mean  = vel_stack[vel_valid][:train_count].mean()\nvel_std   = vel_stack[vel_valid][:train_count].std() + 1e-6\n\nprint(\"temp_mean, temp_std:\", float(temp_mean), float(temp_std))\nprint(\"vel_mean, vel_std:\", float(vel_mean), float(vel_std))\n\n# ---- Normalize inputs ----\nsea_temp_norm = (sea_temp - temp_mean) / temp_std\nu_norm        = (u_field - vel_mean) / vel_std\nv_norm        = (v_field - vel_mean) / vel_std\n\n\nnum_times, lat_count, lon_count = sea_temp_norm.shape\nprint(\"Normalized shapes:\", sea_temp_norm.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T22:35:47.431598Z","iopub.execute_input":"2025-12-18T22:35:47.432101Z","iopub.status.idle":"2025-12-18T22:35:51.558432Z","shell.execute_reply.started":"2025-12-18T22:35:47.432075Z","shell.execute_reply":"2025-12-18T22:35:51.557771Z"}},"outputs":[{"name":"stdout","text":"Total spatial pixels: 16384\nValid ocean pixels (common mask): 10968.0\ntemp_mean, temp_std: 25.112150192260742 1.4636705384979247\nvel_mean, vel_std: -0.0091839749366045 0.12332417233085632\nNormalized shapes: (3654, 128, 128)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"\n\nplt.figure(figsize=(6, 5))\nim = plt.imshow(valid_2d, origin=\"lower\", cmap=\"Greys\")\nplt.title(\"Common valid mask (1 = ocean, 0 = land/NaN)\")\nplt.xlabel(\"longitude index\")\nplt.ylabel(\"latitude index\")\nplt.colorbar(im, label=\"valid\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T22:36:23.902082Z","iopub.execute_input":"2025-12-18T22:36:23.902852Z","iopub.status.idle":"2025-12-18T22:36:24.215773Z","shell.execute_reply.started":"2025-12-18T22:36:23.902828Z","shell.execute_reply":"2025-12-18T22:36:24.215183Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 600x500 with 2 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"'''\nThis code was used to check that the nan-mask is almost the same for temp and vel arrays for all t.\nThis allows us to use the same 2d nan mask.\n'''\n\n'''\nimport xarray as xr\nimport numpy as np\n\n# Point this to your big file, e.g.:\n# DATA_PATH = \"glorys.nc\"\nds = xr.open_dataset(DATA_PATH)\n\nnum_times = ds.dims[\"time\"]\nchunk_size = 8   # adjust if you want, this keeps memory small\n\ndiff_3d = 0                  # how many (t,lat,lon) points differ\ntemp_mask_2d = None          # will hold \"valid at all times\" for temp\nvel_mask_2d = None           # same for velocity\n\nfor start in range(0, num_times, chunk_size):\n    end = min(start + chunk_size, num_times)\n    print(f\"Processing time indices [{start}:{end})\")\n\n    # Load a small time chunk into memory\n    temp_chunk = ds[\"thetao\"].isel(depth=0, time=slice(start, end)).values  # (chunk, lat, lon)\n    u_chunk    = ds[\"uo\"].isel(depth=0, time=slice(start, end)).values\n    v_chunk    = ds[\"vo\"].isel(depth=0, time=slice(start, end)).values\n\n    # Finite-value masks for this chunk\n    temp_mask_chunk = np.isfinite(temp_chunk)\n    vel_mask_chunk  = np.isfinite(u_chunk) & np.isfinite(v_chunk)\n\n    # --- 3D comparison for this chunk ---\n    diff_3d += np.count_nonzero(temp_mask_chunk != vel_mask_chunk)\n\n    # --- Update 2D \"valid for all times\" masks ---\n    # For this chunk, a cell is valid if it's valid at every time in the chunk\n    temp_mask_2d_chunk = temp_mask_chunk.all(axis=0)   # (lat, lon)\n    vel_mask_2d_chunk  = vel_mask_chunk.all(axis=0)\n\n    if temp_mask_2d is None:\n        # First chunk: initialize with this chunk's \"all times\" mask\n        temp_mask_2d = temp_mask_2d_chunk\n        vel_mask_2d  = vel_mask_2d_chunk\n    else:\n        # Later chunks: must also be valid in all previous chunks\n        temp_mask_2d &= temp_mask_2d_chunk\n        vel_mask_2d  &= vel_mask_2d_chunk\n\n# After streaming through all time chunks:\nsame_3d = (diff_3d == 0)\nsame_2d = np.array_equal(temp_mask_2d, vel_mask_2d)\ndiff_2d = np.count_nonzero(temp_mask_2d != vel_mask_2d)\n\nprint(\"Do temp and vel masks match at every (t,lat,lon)?\", same_3d)\nprint(\"Number of differing points in full 3D mask:\", diff_3d)\nprint(\"Do temp and vel masks match as 2D spatial masks?\", same_2d)\nprint(\"Number of differing pixels in 2D mask:\", diff_2d)\nprint(\"Total valid ocean pixels (2D) in temp:\", temp_mask_2d.sum())\nprint(\"Total valid ocean pixels (2D) in vel:\", vel_mask_2d.sum())\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":" time encodings!\n\n# sea_temp_norm, u_norm, v_norm: shapes (T, H, W)\nnum_times = sea_temp_norm.shape[0]\n\n\n# create time encodings\ntimes = ds['time'].to_index() \nday_of_year = times.dayofyear.values.astype(np.float32)  # 1..366\n# Normalize to [0, 1)\ndoy_norm = (day_of_year - 1) / 365.0\ntheta = 2 * np.pi * doy_norm\ndoy_sin = np.sin(theta).astype(np.float32)\ndoy_cos = np.cos(theta).astype(np.float32)\ndoy_sin = torch.from_numpy(doy_sin)  # shape [T]\ndoy_cos = torch.from_numpy(doy_cos)  # shape [T]\nT, H, W = sea_temp_norm.shape\ndoy_sin_map = doy_sin[:, None, None]  # (N,1,1)\ndoy_sin_map = np.broadcast_to(doy_sin_map, (num_times, H, W)).astype(np.float32)\ndoy_cos_map = doy_cos[:, None, None]  # (N,1,1)\ndoy_cos_map = np.broadcast_to(doy_cos_map, (num_times, H, W)).astype(np.float32)\n\n\n# We want pairs of the form:\n#   input:  [temp(t-1), temp(t)]         (2, H, W)\n#   output: temp(t + PRED_HORIZON)       (1, H, W)\n# with t running from 1 .. num_times - PRED_HORIZON - 1\n\n# Build temp_in with 2 snapshots\ntemp_in = torch.tensor(\n    np.stack(\n        [doy_sin_map[1:-PRED_HORIZON],\n         doy_cos_map[1:-PRED_HORIZON],\n         sea_temp_norm[0:-PRED_HORIZON-1],  # temp at t-1\n         sea_temp_norm[1:-PRED_HORIZON]],    # temp at t\n        axis=1,  # stack along channel dim -> (N, 2, H, W)\n    ),\n    dtype=torch.float32,\n)\n\n# Build temp_out\ntemp_out = torch.tensor(sea_temp_norm[PRED_HORIZON + 1:], dtype=torch.float32).unsqueeze(1)  # (N, 1, H, W)\n\n\n# vel → vel pairs (two channels): vel(t) → vel(t + PRED_HORIZON)\nvel_in = torch.tensor(\n    np.stack(\n        [u_norm[:-PRED_HORIZON], \n         v_norm[:-PRED_HORIZON]], \n        axis=1\n    ), \n    dtype=torch.float32\n)\n\nvel_out = torch.tensor(np.stack([u_norm[PRED_HORIZON:], v_norm[PRED_HORIZON:]], axis=1), dtype=torch.float32)  # (pairs, 2, H, W)\n\n\n# combined temp & vel input\ntemp_vel_in = torch.tensor(\n    np.stack(\n        [doy_sin_map[1:-PRED_HORIZON],\n         doy_cos_map[1:-PRED_HORIZON],\n         sea_temp_norm[0:-PRED_HORIZON-1],  # temp at t-1\n         sea_temp_norm[1:-PRED_HORIZON],    # temp at t\n         u_norm[1:-PRED_HORIZON], \n         v_norm[1:-PRED_HORIZON]], \n        axis=1,  # stack along channel dim -> (N, 2, H, W)\n    ),\n    dtype=torch.float32,\n)\n\n# Number of (input, output) pairs is now:\npairs = temp_in.shape[0]\n\ntrain_count = int(pairs * 0.7)\nval_count = int(pairs * 0.15)\ntrain_idx = slice(0, train_count)\nval_idx = slice(train_count, train_count + val_count)\ntest_idx = slice(train_count + val_count, pairs)\n\nprint('Prediction horizon (steps):', PRED_HORIZON)\nprint('Temp pair shape', temp_in.shape, temp_out.shape) \nprint('Vel pair shape', vel_in.shape, vel_out.shape)    \nprint('Temp vel input shape', temp_vel_in.shape)  \n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T22:36:58.886173Z","iopub.execute_input":"2025-12-18T22:36:58.886473Z","iopub.status.idle":"2025-12-18T22:37:01.425545Z","shell.execute_reply.started":"2025-12-18T22:36:58.886450Z","shell.execute_reply":"2025-12-18T22:37:01.424730Z"}},"outputs":[{"name":"stdout","text":"Prediction horizon (steps): 7\nTemp pair shape torch.Size([3646, 4, 128, 128]) torch.Size([3646, 1, 128, 128])\nVel pair shape torch.Size([3647, 2, 128, 128]) torch.Size([3647, 2, 128, 128])\nTemp vel input shape torch.Size([3646, 6, 128, 128])\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"temp_train = TensorDataset(temp_in[train_idx], temp_out[train_idx])\ntemp_val = TensorDataset(temp_in[val_idx], temp_out[val_idx])\ntemp_test = TensorDataset(temp_in[test_idx], temp_out[test_idx])\n\nvel_train = TensorDataset(vel_in[train_idx], vel_out[train_idx])\nvel_val = TensorDataset(vel_in[val_idx], vel_out[val_idx])\nvel_test = TensorDataset(vel_in[test_idx], vel_out[test_idx])\n\ntemp_vel_train = TensorDataset(temp_vel_in[train_idx], temp_out[train_idx])\ntemp_vel_val = TensorDataset(temp_vel_in[val_idx], temp_out[val_idx])\ntemp_vel_test = TensorDataset(temp_vel_in[test_idx], temp_out[test_idx])\n\nloader_temp_train = DataLoader(temp_train, batch_size=BATCH_SIZE, shuffle=True)\nloader_temp_val = DataLoader(temp_val, batch_size=BATCH_SIZE)\nloader_temp_test = DataLoader(temp_test, batch_size=BATCH_SIZE)\n\nloader_vel_train = DataLoader(vel_train, batch_size=BATCH_SIZE, shuffle=True)\nloader_vel_val = DataLoader(vel_val, batch_size=BATCH_SIZE)\nloader_vel_test = DataLoader(vel_test, batch_size=BATCH_SIZE)\n\nloader_temp_vel_train = DataLoader(temp_vel_train, batch_size=BATCH_SIZE, shuffle=True)\nloader_temp_vel_val = DataLoader(temp_vel_val, batch_size=BATCH_SIZE)\nloader_temp_vel_test = DataLoader(temp_vel_test, batch_size=BATCH_SIZE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T22:37:35.865816Z","iopub.execute_input":"2025-12-18T22:37:35.866335Z","iopub.status.idle":"2025-12-18T22:37:35.874517Z","shell.execute_reply.started":"2025-12-18T22:37:35.866314Z","shell.execute_reply":"2025-12-18T22:37:35.873823Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"\n\ndef mse_loss(pred, target):\n    \"\"\"\n    Loss for temperature models: MSE over valid ocean points.\n    pred, target: (B, 1, H, W)\n    \"\"\"\n    diff2 = (pred - target) ** 2\n    diff2 = diff2 * VALID_MASK\n\n\n    # number of valid pixels per sample\n    pixels_per_sample = VALID_MASK.sum()\n\n    # total valid points across the batch\n    total_points = pixels_per_sample * pred.size(0)\n    \n    return diff2.sum() / total_points\n\n\ndef div_loss(pred):\n    div_sq = (divergence(pred) ** 2) * VALID_MASK  # (B,H,W) * (1,1,H,W) -> (B,1,H,W)\n\n    return div_sq.sum() / VALID_MASK.sum()\n\n\ndef divergence(field):\n    u = field[:, 0]\n    v = field[:, 1]\n    du_dx = u[:, :, 2:] - u[:, :, :-2]\n    dv_dy = v[:, 2:, :] - v[:, :-2, :]\n    du_dx = torch.nn.functional.pad(du_dx, (1, 1, 0, 0))\n    dv_dy = torch.nn.functional.pad(dv_dy, (0, 0, 1, 1))\n    return du_dx + dv_dy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T22:37:45.374520Z","iopub.execute_input":"2025-12-18T22:37:45.375107Z","iopub.status.idle":"2025-12-18T22:37:45.380559Z","shell.execute_reply.started":"2025-12-18T22:37:45.375082Z","shell.execute_reply":"2025-12-18T22:37:45.379868Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"\n\n\nclass CNNForecastModel(nn.Module):\n    \"\"\"\n    Wraps SimpleCNN and provides:\n      - forward(x) -> prediction\n      - training_step(x, y) -> scalar loss using temp_loss\n    \"\"\"\n    def __init__(self, in_channels, out_channels, hidden, lr, div_weight=0):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Conv2d(in_channels, hidden, 3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(hidden, hidden, 3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(hidden, out_channels, 3, padding=1)\n        )\n\n        self.lr = lr\n        self.div_weight = div_weight\n\n    def forward(self, x):\n        return self.net(x)\n\n    def training_step_temp(self, xin, y):\n        \"\"\"\n        xin: [B, 2, H, W]  (temp(t-1), temp(t))\n        y:   [B, 1, H, W]  (temp(t+H))\n        \"\"\"\n        preds = self.forward(xin)\n        return mse_loss(preds, y)\n\n    \n    def training_step_vel(self, xin, y):\n        \"\"\"\n        xin: [B, 2, H, W]  (temp(t-1), temp(t))\n        y:   [B, 1, H, W]  (temp(t+H))\n        \"\"\"\n        preds = self.forward(xin)\n        return mse_loss(preds, y) + div_loss(preds) * self.div_weight\n\n    def parameters_to_optimize(self):\n        return self.net.parameters()\n\n\n\nimport torch.nn.functional as F\nfrom diffusers import UNet2DModel, DDPMScheduler, DDIMScheduler, DPMSolverMultistepScheduler\n\n\nclass DiffusionForecastModel(nn.Module):\n    def __init__(\n        self,\n        cond_channels: int,\n        target_channels: int,\n        block_out_channels=(64, 128, 256, 256),\n        layers_per_block: int = 2,\n        use_attention: bool = True,\n        num_train_timesteps: int = 1000,\n        beta_start: float = 1e-4,\n        beta_end: float = 2e-2,\n        beta_schedule: str = \"squaredcos_cap_v2\",\n        num_inference_steps: int | None = None,\n        dropout: float = 0.0,\n        scheduler_class = DDPMScheduler,\n\n        lr:float = 1e-4,\n        div_weight:float = 0\n    ):\n        super().__init__()\n\n        self.cond_channels = cond_channels      # 2\n        self.target_channels = target_channels  # 1\n\n        in_channels = cond_channels + target_channels  # 2 + 1 = 3\n\n        if use_attention and len(block_out_channels) >= 1:\n            down_block_types = (\n                (\"DownBlock2D\",) * (len(block_out_channels) - 1)\n                + (\"AttnDownBlock2D\",)\n            )\n            up_block_types = (\n                (\"AttnUpBlock2D\",)\n                + (\"UpBlock2D\",) * (len(block_out_channels) - 1)\n            )\n        else:\n            down_block_types = (\"DownBlock2D\",) * len(block_out_channels)\n            up_block_types = (\"UpBlock2D\",) * len(block_out_channels)\n\n        self.unet = UNet2DModel(\n            sample_size=None,\n            in_channels=in_channels,\n            out_channels=target_channels,\n            layers_per_block=layers_per_block,\n            block_out_channels=block_out_channels,\n            down_block_types=down_block_types,\n            up_block_types=up_block_types,\n            num_train_timesteps=num_train_timesteps,\n            dropout=dropout,\n        )\n\n        self.scheduler = scheduler_class(\n            num_train_timesteps=num_train_timesteps,\n            beta_start=beta_start,\n            beta_end=beta_end,\n            beta_schedule=beta_schedule,\n        )\n\n        self.num_train_timesteps = num_train_timesteps\n        self.num_inference_steps = num_inference_steps or num_train_timesteps\n\n        # factor by which spatial dims are effectively divided (2^(#downs))\n        self._down_factor = 2 ** (len(block_out_channels) - 1)\n\n        self.lr = lr\n        self.div_weight = div_weight\n\n    def parameters_to_optimize(self):\n        return self.unet.parameters()\n\n    # ---- helper: pad tensors to be compatible with UNet, return original H,W ----\n    def _pad_for_unet(self, *tensors):\n        \"\"\"\n        Pads all tensors on bottom/right so H,W become multiples of self._down_factor.\n        Returns (padded_tensors, orig_H, orig_W).\n        \"\"\"\n        H = tensors[0].shape[-2]\n        W = tensors[0].shape[-1]\n        f = self._down_factor\n\n        pad_H = (f - H % f) % f\n        pad_W = (f - W % f) % f\n\n        if pad_H == 0 and pad_W == 0:\n            return tensors, H, W\n\n        pad = (0, pad_W, 0, pad_H)  # (left, right, top, bottom) in F.pad\n        padded = [F.pad(t, pad) for t in tensors]\n        return padded, H, W\n\n    # ---------- training: noise prediction ----------\n    def training_step_temp(self, xin, y):\n        device = y.device\n        B = y.shape[0]\n\n        # pad xin and y together\n        (xin_p, y_p), H, W = self._pad_for_unet(xin, y)\n\n        noise_p = torch.randn_like(y_p)\n        timesteps = torch.randint(\n            0,\n            self.scheduler.config.num_train_timesteps,\n            (B,),\n            device=device,\n            dtype=torch.long,\n        )\n\n        x_noisy_p = self.scheduler.add_noise(\n            original_samples=y_p,\n            noise=noise_p,\n            timesteps=timesteps,\n        )\n\n        model_in = torch.cat([xin_p, x_noisy_p], dim=1)  # [B, 3, H_pad, W_pad]\n        noise_pred_p = self.unet(sample=model_in, timestep=timesteps).sample\n\n        # crop back to original H, W before computing loss\n        noise_pred = noise_pred_p[..., :H, :W]\n        noise      = noise_p[..., :H, :W]\n\n        return mse_loss(noise_pred, noise)\n\n    def training_step_vel(self, xin, y):\n        \"\"\"\n        Special training step for *velocity* diffusion model with divergence penalty.\n        div_weight: scalar weight for divergence penalty\n        \"\"\"\n        device = y.device\n        B = y.shape[0]\n    \n        # 1) pad xin and y\n        (xin_p, y_p), H, W = self._pad_for_unet(xin, y)\n    \n        # 2) sample noise and timesteps, add forward noise to *target* y\n        noise_p = torch.randn_like(y_p)\n        timesteps = torch.randint(\n            0,\n            self.scheduler.config.num_train_timesteps,\n            (B,),\n            device=device,\n            dtype=torch.long,\n        )\n    \n        x_noisy_p = self.scheduler.add_noise(\n            original_samples=y_p,\n            noise=noise_p,\n            timesteps=timesteps,\n        )\n    \n        # 3) predict noise\n        model_in = torch.cat([xin_p, x_noisy_p], dim=1)\n        noise_pred_p = self.unet(sample=model_in, timestep=timesteps).sample\n    \n        # crop back to original H×W\n        noise_pred = noise_pred_p[..., :H, :W]\n        noise      = noise_p[..., :H, :W]\n    \n        # 4) ε-loss (standard diffusion objective) using your masked temp_loss\n        loss_eps = mse_loss(noise_pred, noise)\n    \n        # 5) reconstruct x0_pred from (x_t, ε̂_t, ᾱ_t)\n        #    ᾱ_t from scheduler.alphas_cumprod\n        #    NOTE: alphas_cumprod is on CPU by default, move to device\n        alphas_cumprod = self.scheduler.alphas_cumprod.to(device)\n        alpha_bar = alphas_cumprod[timesteps].view(B, 1, 1, 1)  # [B,1,1,1]\n    \n        sqrt_alpha_bar = torch.sqrt(alpha_bar)\n        sqrt_one_minus_alpha_bar = torch.sqrt(1.0 - alpha_bar)\n    \n        # use padded tensors for correct shapes, then crop\n        x0_pred_p = (x_noisy_p - sqrt_one_minus_alpha_bar * noise_pred_p) / sqrt_alpha_bar\n        x0_pred   = x0_pred_p[..., :H, :W]   # predicted clean vel [B,2,H,W]\n        y_clean   = y                        # already [B,2,H,W]\n    \n        # 6) divergence-penalized div loss on x0_pred vs y\n        loss_div = div_loss(x0_pred)\n    \n        # 7) total loss: noise MSE + vel/divergence loss\n        loss_total = loss_eps + loss_div * self.div_weight\n        return loss_total\n\n    \n    # ---------- inference: xin -> forecast ŷ ----------\n    def forward(self, xin):\n        return self.forward_ex(xin, output_samples=None)\n        \n    def forward_ex(self, xin, output_samples):\n        device = xin.device\n        B, _, H, W = xin.shape\n\n        (xin_p,), H_orig, W_orig = self._pad_for_unet(xin)\n        B, _, H_pad, W_pad = xin_p.shape\n\n        x = torch.randn(B, self.target_channels, H_pad, W_pad, device=device)\n\n        self.scheduler.set_timesteps(self.num_inference_steps)\n        self.scheduler.timesteps = self.scheduler.timesteps.to(device)\n\n        \n        \n        for t in self.scheduler.timesteps:\n            model_in = torch.cat([xin_p, x], dim=1)\n            noise_pred = self.unet(sample=model_in, timestep=t).sample\n\n            step = self.scheduler.step(\n                model_output=noise_pred,\n                timestep=t,\n                sample=x,\n            )\n            x = step.prev_sample\n\n            if output_samples is not None:\n                output_samples.append(x[..., :H_orig, :W_orig])\n\n        # crop back to original spatial size\n        x = x[..., :H_orig, :W_orig]\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T22:39:49.554177Z","iopub.execute_input":"2025-12-18T22:39:49.555414Z","iopub.status.idle":"2025-12-18T22:39:49.575176Z","shell.execute_reply.started":"2025-12-18T22:39:49.555377Z","shell.execute_reply":"2025-12-18T22:39:49.574578Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"\ndef train_epoch_temp(model, loader, optimizer):\n    model.train()\n    losses = []\n\n    for xin, y in loader:\n        xin = xin.to(DEVICE)\n        y = y.to(DEVICE)\n\n        optimizer.zero_grad()\n        loss = model.training_step_temp(xin, y)\n        loss.backward()\n        optimizer.step()\n\n        losses.append(loss.item())\n\n    return sum(losses) / len(losses)\n\n@torch.no_grad()\ndef eval_epoch(model, loader):\n    \"\"\"\n    Compute the *training objective* on the validation set,\n    reusing model.training_step, but without any training.\n    Works for both CNNForecastModel and DiffusionForecastModel.\n    \"\"\"\n    model.eval()\n    losses = []\n\n    for xin, y in loader:\n        xin = xin.to(DEVICE)\n        y   = y.to(DEVICE)\n\n        loss = model.training_step_temp(xin, y)\n        losses.append(loss.item())\n\n    return sum(losses) / len(losses)\n\n\ndef train_epoch_vel(model, loader, optimizer):\n    model.train()\n    losses = []\n\n    for xin, y in loader:\n        xin = xin.to(DEVICE)\n        y   = y.to(DEVICE)\n\n        optimizer.zero_grad()\n        loss = model.training_step_vel(xin, y)\n        loss.backward()\n        optimizer.step()\n\n        losses.append(loss.item())\n\n    return sum(losses) / len(losses)\n\n@torch.no_grad()\ndef evaluate_model_rmse(model, loader):\n    model.eval()\n    mses = []\n    for xin, y in loader:\n        xin = xin.to(DEVICE)\n        y = y.to(DEVICE)\n        preds = model(xin)\n        mses.append(mse_loss(preds, y))  # uses global VALID_MASK inside rmse\n    mean_mse = sum(mses) / len(mses)\n    return torch.sqrt(mean_mse)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T22:39:53.862311Z","iopub.execute_input":"2025-12-18T22:39:53.862991Z","iopub.status.idle":"2025-12-18T22:39:53.870189Z","shell.execute_reply.started":"2025-12-18T22:39:53.862969Z","shell.execute_reply":"2025-12-18T22:39:53.869528Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"\n\ntemp_candidates = [\n    # ('diffusion1', #!!! the name\n    #  lambda: DiffusionForecastModel(\n    #      cond_channels=4,\n    #      target_channels=1,\n    #      lr = 1e-4,\n\n    #      #!!! change hyperparams below\n    #      block_out_channels=(32, 32), #\n    #      use_attention=False,  #\n    #      num_train_timesteps=1000, #\n    #      num_inference_steps=50, #\n    #      beta_schedule=\"squaredcos_cap_v2\", #\n    #      scheduler_class=DDIMScheduler, #\n    #      dropout=0.2, #\n    #  )),\n    # ('temp_cnn_1',\n    #  lambda: CNNForecastModel(in_channels=4, out_channels=1, hidden=16, lr = 1e-4)),\n    # ('diffusion2',\n    #  lambda: DiffusionForecastModel(\n    #      cond_channels=4,    # xin channels\n    #      target_channels=1,  # y channels\n    #      block_out_channels=(32, 64, 64),\n    #      use_attention=False,\n    #      num_train_timesteps=1000,\n    #      num_inference_steps=50,\n    #      beta_schedule=\"squaredcos_cap_v2\",\n    #      scheduler_class=DDPMScheduler,\n    #      dropout=0.2,\n    #      lr = 1e-4,\n    #  )),\n    # ('diffusion3',\n    #  lambda: DiffusionForecastModel(\n    #      cond_channels=4,    # xin channels\n    #      target_channels=1,  # y channels\n    #      block_out_channels=(32, 64, 64),\n    #      use_attention=False,\n    #      num_train_timesteps=1000,\n    #      num_inference_steps=50,\n    #      beta_schedule=\"squaredcos_cap_v2\",\n    #      scheduler_class=DDIMScheduler,\n    #      dropout=0.2,\n    #      lr = 1e-4,\n    #  )),\n    ('diffusion4',\n     lambda: DiffusionForecastModel(\n         cond_channels=4,    # xin channels\n         target_channels=1,  # y channels\n         block_out_channels=(32, 64, 64),\n         use_attention=False,\n         num_train_timesteps=1000,\n         num_inference_steps=50,\n         beta_schedule=\"squaredcos_cap_v2\",\n         scheduler_class=DPMSolverMultistepScheduler,\n         dropout=0.2,\n         lr = 1e-4,\n     )),\n]\n\n\n# take 1 vel snapshot and output 1 vel snapshot prediction PRED_HORIZON days in future\nvel_candidates = [\n    # (\n    #     'vel_cnn_1',\n    #     lambda: CNNForecastModel(\n    #         in_channels=2,   # [u(t), v(t)]\n    #         out_channels=2,  # [u(t+H), v(t+H)]\n    #         hidden=16,\n    #         lr = 1e-4,\n    #         div_weight = 0.5\n    #     ),\n    # ),\n    # (\n    #     'diffusion32',\n    #     lambda: DiffusionForecastModel(\n    #         cond_channels=2,   # xin: 2 vel channels\n    #         target_channels=2, # predict 2 vel channels in the future\n    #         block_out_channels=(16, 16),\n    #         use_attention=False,\n    #         num_train_timesteps=1000,\n    #         num_inference_steps=50,\n    #         beta_schedule=\"squaredcos_cap_v2\",\n    #         dropout=0.2,\n    #         lr = 1e-4,\n    #         div_weight = 0.5\n    #     ),\n    # ),\n    # ('vel_diffusion1', #!!! the name\n    #  lambda: DiffusionForecastModel(\n    #      cond_channels=2,\n    #      target_channels=2,\n    #      lr = 1e-4,\n\n    #      #!!! change hyperparams below\n    #      block_out_channels=(32, 32), #\n    #      use_attention=False,  #\n    #      num_train_timesteps=1000, #\n    #      num_inference_steps=50, #\n    #      beta_schedule=\"squaredcos_cap_v2\", #\n    #      scheduler_class=DDIMScheduler, #\n    #      dropout=0.2, #\n    #  )),\n    ('vel_diffusion2', \n    lambda: DiffusionForecastModel(\n        cond_channels=2,\n        target_channels=2,\n        lr=3e-4,     \n        \n        \n        block_out_channels=(64, 128, 128),#  +1 block vs diffusion2\n        use_attention=False,              # still off, isolate effect of depth\n        num_train_timesteps=1000,\n        num_inference_steps=100, # more steps → better denoising, same train cost\n        beta_schedule=\"linear\",\n        scheduler_class=DDPMScheduler,    # keep; already better than DDIM in your setup\n        dropout=0.05,                      # reduce vs 0.1 to stop over-regularizing\n    ))\n]\n\n\ntemp_vel_candidates = [\n    # ('temp_vel_diffusion1', #!!! the name\n    #  lambda: DiffusionForecastModel(\n    #      cond_channels=6,\n    #      target_channels=1,\n    #      lr = 1e-4,\n\n    #      #!!! change hyperparams below\n    #      block_out_channels=(32, 32), #\n    #      use_attention=False,  #\n    #      num_train_timesteps=1000, #\n    #      num_inference_steps=50, #\n    #      beta_schedule=\"squaredcos_cap_v2\", #\n    #      scheduler_class=DDIMScheduler, #\n    #      dropout=0.2, #\n    #  )),\n    ('temp_vel_diffusion2', \n    lambda: DiffusionForecastModel(\n        cond_channels=6,\n        target_channels=1,\n        lr=3e-4,     \n        \n        \n        block_out_channels=(64, 128, 128),#  +1 block vs diffusion2\n        use_attention=False,              # still off, isolate effect of depth\n        num_train_timesteps=1000,\n        num_inference_steps=100, # more steps → better denoising, same train cost\n        beta_schedule=\"linear\",\n        scheduler_class=DDPMScheduler,    # keep; already better than DDIM in your setup\n        dropout=0.05,                      # reduce vs 0.1 to stop over-regularizing\n    ))\n]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T22:40:09.719486Z","iopub.execute_input":"2025-12-18T22:40:09.719795Z","iopub.status.idle":"2025-12-18T22:40:09.727250Z","shell.execute_reply.started":"2025-12-18T22:40:09.719774Z","shell.execute_reply":"2025-12-18T22:40:09.726488Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"temp_results = []\nfor name, build_model in temp_candidates:\n    model = build_model().to(DEVICE)\n    optimizer = torch.optim.Adam(\n        model.parameters_to_optimize(),\n        lr=model.lr,\n    )\n    \n    \n    print(f\"{name} \", end='')\n    train_losses = []\n    val_losses = []\n    for epoch in range(0, NUM_EPOCHS):\n        train_loss = train_epoch_temp(model, loader_temp_train, optimizer)\n        train_losses.append(train_loss)\n    \n        val_loss = eval_epoch(model, loader_temp_val)\n        val_losses.append(val_loss)\n    \n        print(f'train loss: {train_loss}')\n        print(f'val loss: {val_loss}')\n    \n        if epoch % 50 == 0:\n            timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n            path = f'models/{name}_{timestamp}_{epoch}_{val_loss:.4f}_.torch'\n            torch.save(model.state_dict(), path)\n            torch.save((train_losses,val_losses),f'models/{name}_{timestamp}_{epoch}_losses.torch')\n    print()\n\n    temp_results.append({\n        'model': model,\n        'val_losses': val_losses,\n        'train_losses': train_losses,\n        'name': name\n    })\n  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T22:40:36.240368Z","iopub.execute_input":"2025-12-18T22:40:36.241119Z","iopub.status.idle":"2025-12-18T22:41:26.493840Z","shell.execute_reply.started":"2025-12-18T22:40:36.241097Z","shell.execute_reply":"2025-12-18T22:41:26.492832Z"}},"outputs":[{"name":"stdout","text":"diffusion4 train loss: 0.4315446900203824\nval loss: 0.18587711921760014\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/1663968258.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mtimestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%Y%m%d-%H%M%S\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'models/{name}_{timestamp}_{epoch}_{val_loss:.4f}_.torch'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34mf'models/{name}_{timestamp}_{epoch}_losses.torch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m             _save(\n\u001b[1;32m    945\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    808\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m         \u001b[0mcontainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_zipfile_writer_buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    779\u001b[0m             )\n\u001b[1;32m    780\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 781\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_compute_crc32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Parent directory models does not exist."],"ename":"RuntimeError","evalue":"Parent directory models does not exist.","output_type":"error"}],"execution_count":19},{"cell_type":"code","source":"vel_results = []\n\n\nfor name, build_model in vel_candidates:\n    model = build_model().to(DEVICE)\n    optimizer = torch.optim.Adam(\n        model.parameters_to_optimize(),\n        lr=model.lr,\n    )\n    \n    \n    print(f\"{name} \", end='')\n    train_losses = []\n    val_losses = []\n    for epoch in range(0, NUM_EPOCHS+1):\n        train_loss = train_epoch_vel(model, loader_vel_train, optimizer)\n        train_losses.append(train_loss)\n    \n        val_loss = eval_epoch(model, loader_vel_val)\n        val_losses.append(val_loss)\n    \n        print(f'train loss: {train_loss}')\n        print(f'val loss: {val_loss}')\n    \n        if epoch % 10 == 0:\n            timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n            path = f'models/{name}_{timestamp}_{epoch}_{val_loss:.4f}_.torch'\n            torch.save(model.state_dict(), path)\n            torch.save((train_losses,val_losses),f'models/{name}_{timestamp}_{epoch}_losses.torch')\n    print()\n\n    vel_results.append({\n        'model': model,\n        'val_losses': val_losses,\n        'train_losses': train_losses,\n        'name': name\n    })\n  ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"temp_vel_results = []\n\nfor name, build_model in temp_vel_candidates:\n    model = build_model().to(DEVICE)\n    optimizer = torch.optim.Adam(\n        model.parameters_to_optimize(),\n        lr=model.lr,\n    )\n    \n    \n    print(f\"{name} \", end='')\n    train_losses = []\n    val_losses = []\n    for epoch in range(0, NUM_EPOCHS+1):\n        train_loss = train_epoch_temp(model, loader_temp_vel_train, optimizer)\n        train_losses.append(train_loss)\n    \n        val_loss = eval_epoch(model, loader_temp_vel_val)\n        val_losses.append(val_loss)\n    \n        print(f'train loss: {train_loss}')\n        print(f'val loss: {val_loss}')\n    \n        if epoch % 10 == 0:\n            timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n            path = f'models/{name}_{timestamp}_{epoch}_{val_loss:.4f}_.torch'\n            torch.save(model.state_dict(), path)\n            torch.save((train_losses,val_losses),f'models/{name}_{timestamp}_{epoch}_losses.torch')\n    print()\n\n    temp_vel_results.append({\n        'model': model,\n        'val_losses': val_losses,\n        'train_losses': train_losses,\n        'name': name\n    })","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n\nfor result in temp_results:\n    model = result['model']\n    test_rmse = evaluate_model_rmse(model, loader_temp_test)\n\n    print(result['name'], test_rmse)\n\nfor result in vel_results:\n    model = result['model']\n    test_rmse = evaluate_model_rmse(model, loader_vel_test)\n\n    print(result['name'], test_rmse)\n\nfor result in temp_vel_results:\n    model = result['model']\n    test_rmse = evaluate_model_rmse(model, loader_temp_vel_test)\n\n    print(result['name'], test_rmse)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def advection_step(temp, vel_now, dt=ADVECTION_DT, kappa=DIFFUSIVITY):\n    \"\"\"\n    One explicit advection–diffusion step for temperature.\n\n    Inputs:\n      temp     : (1, H, W)   temperature at time t\n      vel_now  : (2, H, W)   velocity at time t      (u_now, v_now)\n      dt       : time step (ADVECTION_DT)\n      kappa    : diffusivity constant (DIFFUSIVITY)\n\n    Returns:\n      temp_next: (1, H, W)   updated temperature field after one step\n    \"\"\"\n    # unpack velocities into components\n    u_now, v_now = vel_now        # each (H, W)\n\n    # spatial gradients of temperature (central differences)\n    dTdx = torch.zeros_like(temp)\n    dTdy = torch.zeros_like(temp)\n\n    # temp is (1, H, W): index last two dims for space\n    dTdx[:, :, 1:-1] = (temp[:, :, 2:] - temp[:, :, :-2]) * 0.5\n    dTdy[:, 1:-1, :] = (temp[:, 2:, :] - temp[:, :-2, :]) * 0.5\n\n    # Laplacian of temperature for diffusion term\n    lap = torch.zeros_like(temp)\n    lap[:, :, 1:-1] = temp[:, :, 2:] - 2 * temp[:, :, 1:-1] + temp[:, :, :-2]\n    lap[:, 1:-1, :] += temp[:, 2:, :] - 2 * temp[:, 1:-1, :] + temp[:, :-2, :]\n\n    # advection term: -(u · ∇T)\n    adv = - (u_now.unsqueeze(0) * dTdx + v_now.unsqueeze(0) * dTdy)\n\n    # diffusion term: κ ∇²T\n    diff_term = kappa * lap\n\n    # explicit Euler update\n    return temp + dt * (adv + diff_term)\n\n\n@torch.no_grad\ndef evaluate_advection(temp_ds, vel_ds):\n    \"\"\"\n    temp_ds items: (t_in, t_out)\n        t_in:  (B, 4, H, W) = [time encoding 1, time encoding 2, temp(t-1), temp(t)]\n        t_out: (1, H, W)   = temp(t+H)\n\n    vel_ds items: (v_in, v_out)\n        v_in:  (2, H, W)   = [u(t), v(t)]\n        v_out: (2, H, W)   = [u(t+H), v(t+H)]  (unused here)\n\n    vel_pred: tensor of predicted vel(t+H), shape (N, 2, H, W)\n    \"\"\"\n    mses = []\n    for (t_in, t_out), (v_in, _) in zip(temp_ds, vel_ds):\n        # t_in is (4, H, W): take the *current* snapshot temp(t) as input to advection\n        temp_now = t_in[3:4].to(DEVICE)      # (1, H, W)\n\n        v_in = v_in.to(DEVICE)               # (2, H, W)  = [u(t), v(t)]\n\n        # advection_step expects (1, H, W), (2, H, W)\n        adv = advection_step(temp_now, v_in, 0.2, 0.2).to(DEVICE)   # (1, H, W)\n\n        # rmse expects (B, C, H, W), so add batch dim\n        mses.append(mse_loss(adv.unsqueeze(0), t_out.unsqueeze(0).to(DEVICE)))\n    mean_mse = sum(mses) / len(mses)\n    return torch.sqrt(mean_mse)\n\nadv_rmse = evaluate_advection(temp_test, vel_test)\n\n@torch.no_grad\ndef evaluate_persistence(temp_loader):\n    \"\"\"\n    Baseline: predict temp(t+H) ≈ temp(t).\n    temp_loader yields (t_in, t_out) with:\n        t_in:  (B, 4, H, W) = [time encoding 1, time encoding 2, temp(t-1), temp(t)]\n        t_out: (B, 1, H, W) = temp(t+H)\n    \"\"\"\n    mses = []\n    for t_in, t_out in temp_loader:\n        # use only the current snapshot temp(t)\n        t_in_cur = t_in[:, 3:4].to(DEVICE)   # (B, 1, H, W)\n        t_out = t_out.to(DEVICE)            # (B, 1, H, W)\n        mses.append(mse_loss(t_in_cur, t_out))\n    mean_mse = sum(mses) / len(mses)\n    return torch.sqrt(mean_mse)\n\npersistence_rmse = evaluate_persistence(loader_temp_test)\n\nprint('Advection baseline RMSE', adv_rmse)\nprint(\"Persistence baseline RMSE:\", persistence_rmse)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Pick an example index\nidx = 100  # change if you want a different sample\n\nVALID_MASK_2D = VALID_MASK[0, 0].cpu().numpy().astype(bool)\n\n\ndef temp_to_2d(arr):\n    \"\"\"\n    arr: (H, W) or (1, H, W) or (2, H, W)\n    Returns a numpy 2D array with land masked as NaN.\n    For multi-channel arr, pass arr[c] explicitly.\n    \"\"\"\n    if arr.ndim == 3 and arr.shape[0] == 1:\n        arr2 = arr[0]\n    else:\n        arr2 = arr\n    arr2 = arr2.copy()\n    arr2[~VALID_MASK_2D] = np.nan\n    return arr2\n\n\ndef plot_temp(ax, field_2d, title):\n    im = ax.imshow(field_2d, origin='lower', cmap='viridis')\n    ax.set_title(title)\n    ax.axis('off')\n    plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n\n\ndef vel_to_rgb(vel):\n    \"\"\"\n    vel: (2, H, W) numpy array (u, v)\n    Returns an RGB image where:\n      R ~ u, B ~ v, G = 0, land = gray.\n    Normalizes u and v separately to [0,1].\n    \"\"\"\n    u = vel[0]\n    v = vel[1]\n\n    u_min, u_max = u.min(), u.max()\n    v_min, v_max = v.min(), v.max()\n    u_norm = (u - u_min) / (u_max - u_min + 1e-8)\n    v_norm = (v - v_min) / (v_max - v_min + 1e-8)\n\n    H, W = u.shape\n    rgb = np.zeros((H, W, 3), dtype=np.float32)\n    rgb[..., 0] = u_norm   # red = u\n    rgb[..., 2] = v_norm   # blue = v\n\n    # land = gray\n    rgb[~VALID_MASK_2D] = 0.5\n    return rgb\n\n\ndef plot_vel(ax, vel, title):\n    \"\"\"\n    vel: (2, H, W) numpy array\n    \"\"\"\n    rgb = vel_to_rgb(vel)\n    ax.imshow(rgb, origin='lower')\n    ax.set_title(title)\n    ax.axis('off')\n\n\nwith torch.no_grad():\n    \n    temp_in, temp_out = temp_test[idx]   # temp_in: (2,H,W), temp_out: (1,H,W)\n\n    temp_in_batch = temp_in.unsqueeze(0).to(DEVICE)   # (1,2,H,W)\n    temp_out_batch = temp_out.unsqueeze(0).to(DEVICE) # (1,1,H,W)\n    \n    temp_in_np = temp_in.numpy()      # (2,H,W)\n    temp_out_np = temp_out.numpy()    # (1,H,W)\n    \n    # ----------------- run all models -----------------\n    predictions = []  # list of (name, pred_np) where pred_np is (1,H,W)\n    \n    with torch.no_grad():\n        for result in temp_results:\n            model = result[\"model\"].to(DEVICE)\n            name = result[\"name\"]\n    \n            model.eval()\n            pred = model(temp_in_batch)              # (1,1,H,W)\n            pred_np = pred.cpu().squeeze(0).numpy()  # (1,H,W)\n    \n            predictions.append((name, pred_np))\n    \n    # ----------------- plotting -----------------\n    n_models = len(predictions)\n    n_cols = 2 + n_models  # Input, Target, and one column per model\n    \n    fig, axes = plt.subplots(1, n_cols, figsize=(4 * n_cols, 4))\n    fig.suptitle(f\"Temp models example idx={idx}\", fontsize=14)\n    \n    # Column 0: input temp(t) – use channel 1 (current snapshot)\n    plot_temp(axes[0], temp_to_2d(temp_in_np[3]), \"Input temp(t)\")\n    \n    # Column 1: target temp(t+H)\n    plot_temp(axes[1], temp_to_2d(temp_out_np[0]), \"Target temp(t+H)\")\n    \n    # Remaining columns: each model prediction\n    for j, (name, pred_np) in enumerate(predictions, start=2):\n        plot_temp(axes[j], temp_to_2d(pred_np[0]), f\"Pred {name}\")\n    \n    plt.tight_layout()\n    plt.show()\n\n    # ===================== VEL MODELS =====================\n    # Reuse the same idx as for temperatures\n    vel_in, vel_out = vel_test[idx]   # (2,H,W), (2,H,W)\n    \n    vel_in_batch = vel_in.unsqueeze(0).to(DEVICE)  # (1,2,H,W)\n    vel_in_np = vel_in.numpy()                     # (2,H,W)\n    vel_out_np = vel_out.numpy()                   # (2,H,W)\n    \n    predictions_vel = []  # list of (name, pred_np) where pred_np is (2,H,W)\n    \n    with torch.no_grad():\n        for result in vel_results:   # <-- list of best vel models\n            model = result[\"model\"].to(DEVICE)\n            name = result[\"name\"]\n    \n            model.eval()\n            pred = model(vel_in_batch)               # (1,2,H,W)\n            pred_np = pred.cpu().squeeze(0).numpy()  # (2,H,W)\n    \n            predictions_vel.append((name, pred_np))\n    \n    n_models_vel = len(predictions_vel)\n    n_cols_vel = 2 + n_models_vel  # Input, Target, models\n    \n    fig, axes = plt.subplots(1, n_cols_vel, figsize=(4 * n_cols_vel, 4))\n    fig.suptitle(f\"Vel models example idx={idx}\", fontsize=14)\n    \n    # Col 0: input vel(t)\n    plot_vel(axes[0], vel_in_np, \"Input vel(t)\")\n    \n    # Col 1: target vel(t+H)\n    plot_vel(axes[1], vel_out_np, \"Target vel(t+H)\")\n    \n    # Remaining cols: each model prediction\n    for j, (name, pred_np) in enumerate(predictions_vel, start=2):\n        plot_vel(axes[j], pred_np, f\"Pred {name}\")\n    \n    plt.tight_layout()\n    plt.show()\n\n    # ===================== COMBINED TEMP MODELS =====================\n    # combined_test[idx]: (6,H,W), (1,H,W)\n    comb_in, comb_out = temp_vel_test[idx]\n    \n    comb_in_batch = comb_in.unsqueeze(0).to(DEVICE)   # (1,6,H,W)\n    comb_in_np = comb_in.numpy()                      # (6,H,W)\n    comb_out_np = comb_out.numpy()                    # (1,H,W)\n    \n    # temp(t) is channel 1 in your comment\n    temp_t_from_comb = comb_in_np[3]  # (H,W)\n    \n    predictions_comb = []  # list of (name, pred_np) where pred_np is (1,H,W)\n    \n    with torch.no_grad():\n        for result in temp_vel_results:   # <-- list of best combined temp models\n            model = result[\"model\"].to(DEVICE)\n            name = result[\"name\"]\n    \n            model.eval()\n            pred = model(comb_in_batch)               # (1,1,H,W)\n            pred_np = pred.cpu().squeeze(0).numpy()   # (1,H,W)\n    \n            predictions_comb.append((name, pred_np))\n    \n    n_models_comb = len(predictions_comb)\n    n_cols_comb = 2 + n_models_comb  # Input, Target, models\n    \n    fig, axes = plt.subplots(1, n_cols_comb, figsize=(4 * n_cols_comb, 4))\n    fig.suptitle(f\"Temp vel models example idx={idx}\", fontsize=14)\n    \n    # Col 0: input temp(t) from combined tensor\n    plot_temp(axes[0], temp_to_2d(temp_t_from_comb), \"Input temp(t) (combined)\")\n    \n    # Col 1: target temp(t+H)\n    plot_temp(axes[1], temp_to_2d(comb_out_np[0]), \"Target temp(t+H)\")\n    \n    # Remaining cols: each model prediction\n    for j, (name, pred_np) in enumerate(predictions_comb, start=2):\n        plot_temp(axes[j], temp_to_2d(pred_np[0]), f\"Pred {name}\")\n    \n    plt.tight_layout()\n    plt.show()\n\n    # ===================== ADVECTION BASELINE =====================\n    # Use same sample idx and non-diff vel prediction vel_pred_test_non_diff[idx]\n    t_in_temp, t_out_temp = temp_test[idx]   # (4,H,W), (1,H,W)\n    v_in, _ = vel_test[idx]                  # (2,H,W), (2,H,W)\n\n    temp_now = t_in_temp[3:4].to(DEVICE)     # use current snapshot temp(t), shape (1,H,W)\n    v_in_dev = v_in.to(DEVICE)\n\n    adv_pred = advection_step(temp_now, v_in_dev,.1)  # (1,H,W)\n    adv_pred_np = adv_pred.cpu().numpy()                       # (1,H,W)\n    t_out_temp_np = t_out_temp.numpy()                         # (1,H,W)\n\n    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n    fig.suptitle(f\"Advection baseline example idx={idx}\", fontsize=14)\n\n    plot_temp(axes[0], temp_to_2d(temp_in_np[3]), \"Input temp(t)\")\n    plot_temp(axes[1], temp_to_2d(t_out_temp_np[0]), \"Target temp(t+H)\")\n    plot_temp(axes[2], temp_to_2d(adv_pred_np[0]), \"Pred advection baseline\")\n\n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\nPlot diffusion steps\n'''\n\nSAMPLES_STEP = 10\n\nwith torch.no_grad():\n    \n    temp_in, temp_out = temp_test[idx]   # temp_in: (2,H,W), temp_out: (1,H,W)\n\n    temp_in_batch = temp_in.unsqueeze(0).to(DEVICE)   # (1,2,H,W)\n    temp_out_batch = temp_out.unsqueeze(0).to(DEVICE) # (1,1,H,W)\n    \n    temp_in_np = temp_in.numpy()      # (2,H,W)\n    temp_out_np = temp_out.numpy()    # (1,H,W)\n    \n    # ----------------- run all models -----------------\n    samples = []\n    \n    with torch.no_grad():\n        result = temp_results[0]\n        model = result[\"model\"].to(DEVICE)\n        name = result[\"name\"]\n        \n        model.eval()\n        pred = model.forward_ex(temp_in_batch, samples)\n        #pred_np = pred.cpu().squeeze(0).numpy()  # (1,H,W)\n    \n    # ----------------- plotting -----------------\n    n_cols = 2 + len(samples)//SAMPLES_STEP  # Input, Target, and one column per sample\n    \n    fig, axes = plt.subplots(1, n_cols, figsize=(4 * n_cols, 4))\n    fig.suptitle(f\"temp backward diffusion idx={idx}\", fontsize=14)\n    \n    # Column 0: input temp(t) – use channel 1 (current snapshot)\n    plot_temp(axes[0], temp_to_2d(temp_in_np[3]), \"Input temp(t)\")\n    \n    # Column 1: target temp(t+H)\n    plot_temp(axes[1], temp_to_2d(temp_out_np[0]), \"Target temp(t+H)\")\n    \n    # Remaining columns: each model prediction\n    j = 2\n    for sample_i in range(0, len(samples), SAMPLES_STEP):\n        sample = samples[sample_i]\n        plot_temp(axes[j], temp_to_2d(sample.cpu().squeeze(0).numpy()[0]), f\"{sample_i}\")\n        j += 1\n\n    \n    plt.tight_layout()\n    plt.show()\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_losses(results, title):\n    \"\"\"\n    results: list of dicts with keys 'train_losses', 'val_losses', optional 'name'\n    \"\"\"\n    plt.figure(figsize=(7, 5))\n\n    for i, res in enumerate(results):\n        train_losses = res.get(\"train_losses\", [])\n        val_losses   = res.get(\"val_losses\", [])\n        name = res.get(\"name\", f\"model_{i}\")\n\n        if not train_losses and not val_losses:\n            continue\n\n        # x-axis = epoch numbers starting from 1\n        e_train = range(1, len(train_losses) + 1)\n        e_val   = range(1, len(val_losses) + 1)\n\n        if train_losses:\n            plt.plot(e_train, train_losses, label=f\"{name} train\")\n        if val_losses:\n            plt.plot(e_val, val_losses, linestyle=\"--\", label=f\"{name} val\")\n\n    plt.title(title)\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.grid(True)\n    plt.legend()\n    plt.tight_layout()\n    plt.show()\n\n\n# --- call for each family of models ---\n\nplot_losses(temp_results, \"Temp models: train vs val loss\")\nplot_losses(vel_results, \"Vel models: train vs val loss\")\nplot_losses(temp_vel_results, \"Temp vel models: train vs val loss\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}